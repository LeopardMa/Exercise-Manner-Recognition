{
  "name": "Exercise-manner-recognition",
  "tagline": "This is a report for Exercise Manner Recognition based on the weight-lifting exercises data set, in order to quantify how well the participants perform the exercises. ",
  "body": "---\r\ntitle: \"Exercise Manner Recognition\"\r\nauthor: \"Rubao Ma\"\r\ndate: \"8th Jun. 2016\"\r\noutput: html_document\r\n---\r\n\r\n##1. Executive summary\r\n\r\nThis is a report for Exercise Manner Recognition based on the weight-lifting exercises data set, in order to quantify how well the participants perform the exercises. \r\nThere are many variables in the original data set, but some of them are useless for the model training. \r\nDuring preprocessing, we remove some variables with little information, and employ PCA method to deal with all numerical variables. \r\nAfter that, the model is trained through random forest, and it is proved to be effective in the  validation set. \r\nThe prediction result for the test set is given by this model as well. \r\n\r\n##2. Weight-lifting exercises data set\r\nThis is a data set collected from 6 male participants, who were asked to perform one set of 10 repetitions\r\nof the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). \r\nClass A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes.  \r\nThe training data for this project are available here:  \r\n<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv>  \r\nThe test data are available here:  \r\n<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv>  \r\nEach observation in the data set consists of 160 variables, including the Euler angles (roll, pitch and yaw), as well as the raw accelerometer, gyroscope and magnetometer readings, in addition to eight features calculated form the Euler angles of each of the four sensors: mean, variance, standard deviation, max, min, amplitude, kurtosis and skewness. \r\nIn the training set, there are 19622 rows, while there are 20 rows in testing set.  \r\nLoad data set  \r\n```{r,cache=TRUE}\r\ntraining <- read.csv(\"pml-training.csv\",header = TRUE)\r\ntesting <- read.csv(\"pml-testing.csv\",header = TRUE)\r\ndim(training)\r\ndim(testing)\r\n```\r\n\r\n##3. Preprocessing\r\n\r\nThere a lot of NA values and useless variables in the data sets.  \r\nSuch as \r\n```{r,cache=TRUE}\r\ntable(training$new_window)\r\ntable(testing$new_window)\r\nsum(training$kurtosis_roll_belt==\"\")/length(training$kurtosis_roll_belt)\r\nsum(is.na(training$max_roll_belt))/length(training$max_roll_belt)\r\n\r\n```\r\nFor the variable, new_window, both \"no\" and \"yes\" are present in the training set, but not the same in the test set. \r\nThen, it is a useless variable in this case. \r\nMoreover, some variables in the data sets just get NULL value or \"NA\", which are not the intrested factors for the recognition. \r\nFor instance, NULL value and \"NA\" for kurtosis_roll_belt and max_roll_belt respectively both account for more than 97.93%.  \r\nUnder this situation, we truncate the original data sets.\r\n```{r,cache=TRUE}\r\nna_testing <- (is.na(testing))|(testing==\"\")\r\ntable(colSums(na_testing))\r\n```\r\n100 out of 160 variables in the test set are meaningless so that the corresponding ones in the training set can be removed.\r\n\r\n```{r}\r\nsub_testing1 <- testing[,colSums(na_testing)==0]\r\nsub_training1 <- training[training$new_window==\"no\",colSums(na_testing)==0]\r\n```\r\nBesides, the lable in the first column, all timestamps, new_window and num_window are useless, as well as the problem_id in testing set.\r\n```{r, cache=TRUE}\r\nsub_testing2 <- sub_testing1[,-c(1,3,4,5,6,7,60)]\r\nsub_training2 <- sub_training1[,-c(1,3,4,5,6,7)]\r\n```\r\nNear zero-variance predictors analysis\r\n```{r,message=FALSE,warning=FALSE}\r\nlibrary(caret)\r\nnzv(sub_training2)\r\n```\r\nSo far, the dimensions of the data set become\r\n```{r, cache=TRUE}\r\ndim(sub_testing2)\r\ndim(sub_training2)\r\n```\r\n\r\n## 3. Model training\r\nThere are at least two different kinds of data in the data set. \r\nSpecifically, the value of user_name in the training set is discrete, while most of other value are continuous. \r\nRandom forest is adopted to train the model, and cross validation is used to measure the out of sample error.   \r\nBecause the original training set is sorted by user_name and timestamp, we need to break this order randomly. \r\n```{r, cache=TRUE, message=FALSE, warning=FALSE}\r\nset.seed(12345)\r\nsub_training2 <- sub_training2[sample(1:dim(sub_training2)[1],dim(sub_training2)[1],replace = FALSE),]\r\n```\r\nSplit the training set into two parts, modelBuilding (70%) and validation (30%).\r\n```{r,cache=TRUE}\r\ninTrain <- createDataPartition (y=sub_training2$classe,p=0.7,list=FALSE)\r\nModelBuilding <- sub_training2[inTrain,]\r\nCrossValid <- sub_training2[-inTrain,]\r\n```\r\nPCA method is performed on all variables except user_name, and we construct data set MB_PCA,CV_PCA and Test_PCA for model building, cross validation and test set respectively.  \r\n```{r,cache=TRUE}\r\nPCA <- preProcess(ModelBuilding[,-c(1,54)],method = \"pca\", thresh = 0.9)\r\nMB_PCA <- data.frame(user_name=ModelBuilding$user_name,predict(PCA,ModelBuilding[,-c(1,54)]),classe=ModelBuilding$classe)\r\nCV_PCA <- data.frame(user_name=CrossValid$user_name,predict(PCA,CrossValid[,-c(1,54)]),classe=CrossValid$classe)\r\nTest_PCA <- data.frame(user_name=sub_testing2$user_name,predict(PCA,sub_testing2[,-1]))\r\n```\r\nThe model is trained by ramdon forest using data set MB_PCA.\r\n```{r,cache=TRUE,message=FALSE,warning=FALSE}\r\nfit <- train(classe~.,data=MB_PCA,method=\"rf\", prox=TRUE)\r\n```\r\nModel training is completed.\r\n\r\n## 3. Prediction result  \r\n\r\nThen, the prediction of training set, validation set and test set is \r\n```{r, cache=TRUE}\r\npredict_train <- predict(fit,MB_PCA[,-20])\r\npredict_CV <- predict(fit,CV_PCA[,-20])\r\npredict_test <- predict(fit,Test_PCA)\r\n```\r\nThe in sample error\r\n```{r, cache=TRUE}\r\nconfusionMatrix(predict_train,MB_PCA$classe)\r\n```\r\nAll samples are recognized precisely, and Kappa equals to 1. In terms of the training set, this model is perfect.  \r\nThe out of sample error\r\n```{r, cache=TRUE}\r\nconfusionMatrix(predict_CV,CV_PCA$classe)\r\n```\r\nThe accuracy for validation set is 97%, and Kappa equals to 0.962. \r\nIt means that the model is effective.  \r\nThe prediction for test set is \r\n```{r}\r\ntest_result <- data.frame(problem_id=testing$problem_id,prediction_classe=predict_test)\r\ntest_result\r\n```\r\n\r\n## 4. Conclusion\r\n\r\nIn this work, we complete the task following steps below:  \r\n1. Adjust the structure of the original data set manually according to the characteristics of the data set itself;  \r\n2. Deal with the training set by PCA in order to diminish the impact of variables with strong correlation with each other and reduce the number of variables for model training;  \r\n3. It is taken into consideration that there are two kinds of variable in the data sets, then we choose random forest to train the model;  \r\n4. The model is tested in training set and validation set, which proves its effectiveness;  \r\n5. The prediction of test set is given by the model above.  \r\nThis scheme works to some extent in this work. \r\nIt may get some inprovement by trying other methods to train the model.",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}